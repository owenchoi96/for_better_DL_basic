{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LeNet Implementation with Sequential APIS"
      ],
      "metadata": {
        "id": "mDdMFAWCvhRN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOpZUN6PxCIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten, Dense, ZeroPadding2D\n",
        "\n",
        "LeNet1 = Sequential()\n",
        "\n",
        "LeNet1.add(Conv2D(filters=4, kernel_size=5, padding='valid', strides=1,\n",
        "                  activation='tanh'))\n",
        "LeNet1.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "LeNet1.add(Conv2D(filters=12, kernel_size=5, padding='valid', strides=1,\n",
        "                  activation='tanh'))\n",
        "LeNet1.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "LeNet1.add(Flatten())\n",
        "LeNet1.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# LeNet1.build(input_shape=(None, 28, 28, 1))\n",
        "# LeNet1.summary()\n",
        "\n",
        "# LeNet4\n",
        "LeNet4 = Sequential()\n",
        "LeNet4.add(ZeroPadding2D(padding=2))\n",
        "LeNet4.add(Conv2D(filters=4, kernel_size=5, padding='valid', strides=1,\n",
        "                  activation='tanh'))\n",
        "LeNet4.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "LeNet4.add(Conv2D(filters=16, kernel_size=5, padding='valid', strides=1,\n",
        "                  activation='tanh'))\n",
        "LeNet4.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "LeNet4.add(Flatten())\n",
        "LeNet4.add(Dense(units=120, activation='tanh'))\n",
        "LeNet4.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# LeNet4.build(input_shape=(None, 28, 28, 1))\n",
        "# LeNet4.summary()\n",
        "\n",
        "\n",
        "# LeNet5\n",
        "LeNet5 = Sequential()\n",
        "LeNet5.add(ZeroPadding2D(padding=2))\n",
        "LeNet5.add(Conv2D(filters=6, kernel_size=5, padding='valid', strides=1,\n",
        "                  activation='tanh'))\n",
        "LeNet5.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "LeNet5.add(Conv2D(filters=16, kernel_size=5, padding='valid', strides=1,\n",
        "                  activation='tanh'))\n",
        "LeNet5.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "LeNet5.add(Flatten())\n",
        "LeNet5.add(Dense(units=140, activation='tanh'))\n",
        "LeNet5.add(Dense(units=84, activation='tanh'))\n",
        "LeNet5.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "LeNet5.build(input_shape=(None, 28, 28, 1))\n",
        "LeNet5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEG4EH9xvh2k",
        "outputId": "2823868b-8599-4760-f1e6-c9bb00a7e63a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " zero_padding2d_2 (ZeroPaddi  (None, 32, 32, 1)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " average_pooling2d_15 (Avera  (None, 14, 14, 6)        0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " average_pooling2d_16 (Avera  (None, 5, 5, 16)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 140)               56140     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 84)                11844     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 71,406\n",
            "Trainable params: 71,406\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXQY9SAevh43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LeNet Implementation with Model Sub Classing"
      ],
      "metadata": {
        "id": "gIezQ2E6vh7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten, Dense, ZeroPadding2D\n",
        "\n",
        "class LeNet1(Model):\n",
        "    def __init__(self):\n",
        "        super(LeNet1, self).__init__()\n",
        "\n",
        "        # feature extractor\n",
        "        self.conv1 = Conv2D(filters=4, kernel_size=5, padding='valid', strides=1,\n",
        "                            activation='tanh')\n",
        "        self.conv1_pool = AveragePooling2D(pool_size=2, strides=2)\n",
        "\n",
        "        self.conv2 = Conv2D(filters=12, kernel_size=5, padding='valid', strides=1,\n",
        "                            activation='tanh')\n",
        "        self.conv2_pool = AveragePooling2D(pool_size=2, strides=2)\n",
        "\n",
        "        # classifier \n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(units=10, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        \n",
        "        x = conv1(x)\n",
        "        x = conv1_pool(x)\n",
        "        x = conv2(x)\n",
        "        x = conv2_pool(x)\n",
        "\n",
        "        x = flatten(x)\n",
        "        x = d1(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kJeNgmIjvh-P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LeNet4\n",
        "class LeNet4(Model):\n",
        "    def __init__(self):\n",
        "        super(LeNet4, self).__init__()\n",
        "\n",
        "        # feature extractor\n",
        "        self.zero_padding = ZeroPadding2D(padding=2)\n",
        "        self.conv1 = Conv2D(filters=4, kernel_size=5, padding='valid', strides=1,\n",
        "                            activation='tanh')\n",
        "        self.conv1_pool = AveragePooling2D(pool_size=2, strides=2)\n",
        "\n",
        "        self.conv2 = Conv2D(filters=16, kernel_size=5, padding='valid', strides=1,\n",
        "                            activation='tanh')\n",
        "        self.conv2_pool = AveragePooling2D(pool_size=2, strides=2)\n",
        "\n",
        "        # classifier \n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(units=120, activation='tanh')\n",
        "        self.d2 = Dense(units=10, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = zero_padding(x)\n",
        "        x = conv1(x)\n",
        "        x = conv1_pool(x)\n",
        "        x = conv2(x)\n",
        "        x = conv2_pool(x)\n",
        "\n",
        "        x = flatten(x)\n",
        "        x = d1(x)\n",
        "        x = d2(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "QoP0uFo2viA0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LeNet5\n",
        "class LeNet5(Model):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        # feature extractor\n",
        "        self.zero_padding = ZeroPadding2D(padding=2)\n",
        "        self.conv1 = Conv2D(filters=6, kernel_size=5, padding='valid', strides=1,\n",
        "                            activation='tanh')\n",
        "        self.conv1_pool = AveragePooling2D(pool_size=2, strides=2)\n",
        "\n",
        "        self.conv2 = Conv2D(filters=16, kernel_size=5, padding='valid', strides=1,\n",
        "                            activation='tanh')\n",
        "        self.conv2_pool = AveragePooling2D(pool_size=2, strides=2)\n",
        "\n",
        "        # classifier \n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(units=140, activation='tanh')\n",
        "        self.d2 = Dense(units=84, activation='tanh')\n",
        "        self.d2 = Dense(units=10, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = zero_padding(x)\n",
        "        x = conv1(x)\n",
        "        x = conv1_pool(x)\n",
        "        x = conv2(x)\n",
        "        x = conv2_pool(x)\n",
        "\n",
        "        x = flatten(x)\n",
        "        x = d1(x)\n",
        "        x = d2(x)\n",
        "        x = d3(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "V01WKSd-viDq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sequential + Model\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten, Dense, ZeroPadding2D\n",
        "\n",
        "class LeNet1(Model):\n",
        "    def __init__(self):\n",
        "        super(LeNet1, self).__init__()\n",
        "\n",
        "        # feature extractor\n",
        "        self.fe = Sequential()\n",
        "        self.fe.add(Conv2D(filters=4, kernel_size=5, padding='valid', strides=1,\n",
        "                            activation='tanh'))\n",
        "        self.fe.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "        self.fe.add(Conv2D(filters=12, kernel_size=5, padding='valid', strides=1,\n",
        "                            activation='tanh'))\n",
        "        self.fe.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "\n",
        "        # classifier \n",
        "        self.classifier = Sequential()\n",
        "        self.classifier.add(Flatten())\n",
        "        self.classifier.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fe(x)\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hTe9rCgVviJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer + Model + Sequential\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten, Dense, ZeroPadding2D\n",
        "\n",
        "class FeatureExtractor(Layer):\n",
        "    def __init__(self, filter1, filter2): # 여기에 filter 개수 받기 \n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv1 = Conv2D(filters=filter1, kernel_size=5, padding='valid',\n",
        "                            strides=2, activation='tanh')\n",
        "        self.conv1_pool = AveragePooling2D(pool_size=2, strides=2)\n",
        "        self.conv2 = Conv2D(filters=filter2, kernel_size=5, padding='valid',\n",
        "                            strides=2, activation='tanh')\n",
        "        self.conv2_pool = AveragePooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv1_pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv2_pool(x)\n",
        "        return x\n",
        "\n",
        "class LeNet1(Model):\n",
        "    def __init__(self):\n",
        "        super(LeNet1, self).__init__()\n",
        "\n",
        "        # feature extractor\n",
        "        self.fe = FeatureExtractor(4, 12)\n",
        "\n",
        "        # classifier \n",
        "        self.classifier = Sequential()\n",
        "        self.classifier.add(Flatten())\n",
        "        self.classifier.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fe(x)\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ubetJ-cviLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjEBak8JviOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions - learning env setting1"
      ],
      "metadata": {
        "id": "FMITE-VlviQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "-- project directory\n",
        "    -- main.py\n",
        "    -- models.py\n",
        "    -- utils\n",
        "        -- basic_utils.py\n",
        "        -- cp_utils.py\n",
        "        -- dataset_utils.py\n",
        "        -- learning_env_setting.py\n",
        "        -- train_validation_test.py\n",
        "\n",
        "    -- train1\n",
        "        -- confusion_matrix\n",
        "        -- model\n",
        "        -- loss_accs.npz\n",
        "        -- losses_accs_visualize.png\n",
        "        -- test_result.txt \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gocMju3PviTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
        "\n",
        "def dir_setting(dir_name, CONTINUE_LEARNING):\n",
        "    dir_name = 'train1'\n",
        "    CONTINUE_LEARNING = False\n",
        "\n",
        "                            # 현재 경로   # 만들고자 하는 파일 이름 \n",
        "    cp_path = os.path.join(os.getcwd(), dir_name)\n",
        "    confusion_path = os.path.join(cp_path, 'confusion_matrix')\n",
        "    model_path = os.path.join(cp_path, 'model')\n",
        "\n",
        "    print(cp_path)\n",
        "    print(confusion_path)\n",
        "    print(model_path)\n",
        "\n",
        "                    \n",
        "    if CONTINUE_LEARNING == False and os.path.isdir(cp_path):\n",
        "        shutil.rmtree(cp_path) # 싹 지우고 다시 학습시켜줘. \n",
        "\n",
        "    if not os.path.isdir(cp_path):\n",
        "        os.makedirs(cp_path, exist_ok=True)\n",
        "        os.makedirs(confusion_path, exist_ok=True)\n",
        "        os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    path_dict = {'cp_path':cp_path,\n",
        "                'confusion_path': confusion_path,\n",
        "                'model_path':model_path}\n",
        "\n",
        "    return path_dict \n",
        "\n",
        "\n",
        "def get_classification_metrics():\n",
        "    train_loss = Mean()\n",
        "    train_acc = SparseCategoricalAccuracy()\n",
        "\n",
        "    validation_loss = Mean()\n",
        "    validation_acc = SparseCategoricalAccuracy()\n",
        "\n",
        "    test_loss = Mean()\n",
        "    test_acc = SparseCategoricalAccuracy()\n",
        "\n",
        "    metric_objects = dict()\n",
        "    metric_objects['train_loss'] = train_loss\n",
        "    metric_objects['train_acc'] = train_acc\n",
        "    metric_objects['validation_loss'] = validation_loss\n",
        "    metric_objects['validation_acc'] = validation_acc\n",
        "    metric_objects['test_loss'] = test_loss\n",
        "    metric_objects['test_acc'] = test_acc\n",
        "\n",
        "    return metric_objects\n",
        "\n"
      ],
      "metadata": {
        "id": "C5W2kd4xviWa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6Ha_zsiviYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions - learning env setting2"
      ],
      "metadata": {
        "id": "8o_ovEtAvib2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from termcolor import colored\n",
        "import numpy as np\n",
        "\n",
        "from utils.learning_env_setting import dir_setting  # 파일로 만들어진것\n",
        "\n",
        "dir_name = 'train1'\n",
        "CONTINUE_LEARNING = False\n",
        "\n",
        "path_dict = dir_setting(dir_name, CONTINUE_LEARNING)\n",
        "\n",
        "model = 'test'\n",
        "\n",
        "def continue_setting(CONTINUE_LEARNING, path_dict, model=None):\n",
        "    if CONTINUE_LEARNING == True and len(os.listdir(path_dict['model_path'])) == 0:\n",
        "        CONTINUE_LEARNING = False\n",
        "        print(colored('continue learning flat ahs been converted to false', 'cyan'))\n",
        "\n",
        "    if CONTINUE_LEARNING == True:\n",
        "        epoch_list = os.listdir(path_dict['model_path'])\n",
        "        epoch_list = [int(epoch.split('_')[1] for epoch in epoch_list)]\n",
        "        \n",
        "        last_epoch = epoch_list[-1]\n",
        "        model_path = path_dict['model_path'] + '/epoch_' + str(last_epoch)\n",
        "        model = tf.keras.models.load_model(model_path) # model 불러오기\n",
        "        \n",
        "        losses_accs_path = path_dict['cp_path']\n",
        "        losses_acc_np = np.load(losses_accs_path + './losses_acc.npz')\n",
        "        losses_accs = dict()\n",
        "        for k,v in losses_accs_np.items():\n",
        "            losses_accs[k] = list(v)\n",
        "\n",
        "        start_epoch = last_epoch + 1\n",
        "        \n",
        "    else:\n",
        "        model = model\n",
        "        start_epoch=0\n",
        "        losses_accs = {'train_losses':[], 'train_accs':[],\n",
        "                    'validation_losses':[], 'validation_accs':[]}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0tw2ec0Qviek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3hXaiROVvihK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utilify functions - 3 more utils"
      ],
      "metadata": {
        "id": "5YOqgYZ5vij-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwrbKXrxvimy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rSNJND_Evipl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EOhhvksevisa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rtG7OJjnvivA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-mcE3unvixy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gCL0yJwhvi0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDy9v9hWvi3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}